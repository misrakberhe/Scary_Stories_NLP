{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "89hFhvbGmvH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn9vgt-wl8_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "import feedparser\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import base64\n",
        "from ttp import ttp\n",
        "\n",
        "sys.path.append('/content/grover')\n",
        "from lm.modeling import GroverConfig, sample\n",
        "from sample.encoder import get_encoder, _tokenize_article_pieces, extract_generated_target\n",
        "import random\n",
        "\n",
        "def generate_article_attribute(sess, encoder, tokens, probs, article, target='article'):\n",
        "\n",
        "    \"\"\"\n",
        "    Given attributes about an article (title, author, etc), use that context to generate\n",
        "    a replacement for one of those attributes using the Grover model.\n",
        "    This function is based on the Grover examples distributed with the Grover code.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the raw article text\n",
        "    article_pieces = _tokenize_article_pieces(encoder, article)\n",
        "    print (article_pieces)\n",
        "\n",
        "    # Grab the article elements the model careas about - domain, date, title, etc.\n",
        "    context_formatted = []\n",
        "    for key in ['domain', 'date', 'authors', 'title', 'article']:\n",
        "        if key != target:\n",
        "            context_formatted.extend(article_pieces.pop(key, []))\n",
        "\n",
        "    # Start formatting the tokens in the way the model expects them, starting with\n",
        "    # which article attribute we want to generate.\n",
        "    context_formatted.append(encoder.__dict__['begin_{}'.format(target)])\n",
        "    # Tell the model which special tokens (such as the end token) aren't part of the text\n",
        "    ignore_ids_np = np.array(encoder.special_tokens_onehot)\n",
        "    ignore_ids_np[encoder.__dict__['end_{}'.format(target)]] = 0\n",
        "\n",
        "    # We are only going to generate one article attribute with a fixed\n",
        "    # top_ps cut-off of 95%. This simple example isn't processing in batches.\n",
        "    gens = []\n",
        "    article['top_ps'] = [0.95]\n",
        "\n",
        "    # Run the input through the TensorFlow model and grab the generated output\n",
        "    tokens_out, probs_out = sess.run(\n",
        "        [tokens, probs],\n",
        "        feed_dict={\n",
        "            # Pass real values for the inputs that the\n",
        "            # model needs to be able to run.\n",
        "            initial_context: [context_formatted],\n",
        "            eos_token: encoder.__dict__['end_{}'.format(target)],\n",
        "            ignore_ids: ignore_ids_np,\n",
        "            p_for_topp: np.array([0.95]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # The model is done! Grab the results it generated and format the results into normal text.\n",
        "    for t_i, p_i in zip(tokens_out, probs_out):\n",
        "        extraction = extract_generated_target(output_tokens=t_i, encoder=encoder, target=target)\n",
        "        gens.append(extraction['extraction'])\n",
        "\n",
        "    # Return the generated text.\n",
        "    return gens[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St7ZzM4smLJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article = {\n",
        "    'summary': 'a velvet wrapped blade',\n",
        "    'title': \"the mailbox was full\",\n",
        "    'text': '',\n",
        "    'authors': [],\n",
        "    'publish_date': '04-04-2012',\n",
        "    'iso_date': datetime.now().isoformat(),\n",
        "    'domain': 'nosleep',\n",
        "    'image_url': None,\n",
        "    'tags': None,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kigL1tFnAlmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "06043a4b-96a5-419d-fd63-477e17ba1f93"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "# Load the pre-trained \"huge\" Grover model with 1.5 billion params\n",
        "model_config_fn = '/content/grover/lm/configs/base.json'\n",
        "model_ckpt = 'gs://misrak_capstone/LayerUpdate/model.ckpt-850000'#'/content/model.ckpt-850000'\n",
        "encoder = get_encoder()\n",
        "news_config = GroverConfig.from_json_file(model_config_fn)\n",
        "\n",
        "# Set up TensorFlow session to make predictions\n",
        "tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "\n",
        "with tf.Session(config=tf_config, graph=tf.Graph()) as sess:\n",
        "    # Create the placehodler TensorFlow input variables needed to feed data to Grover model\n",
        "    # to make new predictions.\n",
        "    initial_context = tf.placeholder(tf.int32, [1, None])\n",
        "    p_for_topp = tf.placeholder(tf.float32, [1])\n",
        "    eos_token = tf.placeholder(tf.int32, [])\n",
        "    ignore_ids = tf.placeholder(tf.bool, [news_config.vocab_size])\n",
        "\n",
        "    \n",
        "\n",
        "    # Load the model config to get it set up to match the pre-trained model weights\n",
        "    tokens, probs = sample(\n",
        "        news_config=news_config,\n",
        "        initial_context=initial_context,\n",
        "        eos_token=eos_token,\n",
        "        ignore_ids=ignore_ids,\n",
        "        p_for_topp=p_for_topp,\n",
        "        do_topk=False\n",
        "    )\n",
        "\n",
        "    # Restore the pre-trained Grover model weights\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, model_ckpt)\n",
        "    \n",
        "\n",
        "    print(f\"Building article from headline, summary and text'\")\n",
        "    article['title'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"title\")\n",
        "    article['text'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"article\")\n",
        "    #article['summary'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"summary\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Tensor(\"Placeholder:0\", shape=(1, ?), dtype=int32)\n",
            "Tensor(\"Placeholder_1:0\", shape=(1,), dtype=float32)\n",
            "Tensor(\"Placeholder_2:0\", shape=(), dtype=int32)\n",
            "Tensor(\"Placeholder_3:0\", shape=(50270,), dtype=bool)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from gs://misrak_capstone/LayerUpdate/model.ckpt-850000\n",
            "Building article from headline, summary and text'\n",
            "{'article': [50265, 50266], 'domain': [50257, 39370, 8893, 50258], 'title': [50263, 1170, 37283, 374, 1337, 50264], 'date': [50259, 16785, 8703, 12, 2322, 50260]}\n",
            "{'article': [50265, 50266], 'domain': [50257, 39370, 8893, 50258], 'title': [50263, 28255, 5784, 31, 4519, 17690, 14952, 31, 1401, 735, 1725, 531, 319, 263, 6773, 14, 785, 2751, 32725, 2546, 510, 14, 199, 8, 6192, 26, 310, 7903, 12, 384, 2298, 17690, 9, 417, 2009, 12538, 287, 263, 310, 829, 417, 7624, 12538, 573, 465, 5399, 12764, 12, 384, 2298, 17690, 562, 589, 512, 8459, 656, 285, 1561, 346, 598, 2373, 285, 263, 5094, 12, 288, 263, 1307, 3625, 2746, 14, 8330, 785, 598, 31, 6708, 404, 12, 2976, 31, 16782, 4980, 12, 6389, 31, 315, 13, 75, 649, 5169, 12, 6730, 31, 4163, 408, 357, 1561, 263, 2056, 1224, 315, 13, 52, 13, 51, 12, 315, 13, 52, 13, 41, 3549, 633, 448, 248, 83, 263, 9157, 287, 617, 5722, 14, 199, 15646, 11956, 271, 783, 264, 9154, 285, 3783, 15918, 15918, 8330, 12, 263, 4348, 5028, 294, 4334, 4936, 14, 310, 448, 248, 83, 511, 288, 258, 14240, 13, 36312, 277, 4519, 13, 43, 711, 66, 9999, 463, 16122, 13907, 12, 287, 1782, 12, 285, 5454, 3447, 704, 374, 1017, 285, 308, 2827, 12, 291, 327, 562, 1613, 258, 3017, 735, 1712, 891, 10471, 287, 263, 3453, 1322, 12, 36361, 408, 692, 263, 2762, 357, 6620, 547, 656, 789, 14, 199, 3582, 310, 448, 248, 83, 4707, 785, 315, 9871, 270, 291, 532, 327, 263, 7851, 374, 1017, 285, 1717, 282, 4999, 10482, 287, 664, 899, 14, 771, 1814, 285, 341, 14, 384, 2224, 12, 263, 1172, 285, 19974, 263, 7851, 562, 308, 518, 7896, 622, 263, 3113, 1684, 551, 12, 291, 327, 374, 328, 358, 21384, 9, 263, 650, 2976, 12, 291, 13276, 14, 199, 3582, 384, 1645, 5012, 358, 273, 287, 263, 2259, 315, 448, 248, 298, 7604, 380, 263, 40599, 738, 384, 1389, 35853, 12, 263, 1389, 12077, 5094, 478, 9726, 3757, 285, 263, 650, 12, 285, 13, 1170, 827, 12, 351, 358, 6739, 263, 2521, 12, 291, 1365, 12, 1567, 263, 4519, 13, 43, 711, 4436, 528, 637, 288, 617, 4460, 12, 544, 315, 717, 14, 11590, 12, 352, 258, 10471, 287, 263, 8209, 285, 531, 887, 12, 315, 482, 35744, 263, 2373, 356, 4751, 2583, 2015, 199, 3582, 384, 2613, 401, 22610, 291, 258, 5867, 436, 66, 11393, 583, 12, 1771, 14, 310, 13, 23411, 12, 562, 308, 12873, 285, 32155, 291, 652, 684, 3868, 320, 285, 2976, 14, 686, 37, 84, 270, 84, 9, 1017, 2108, 291, 24885, 14, 2958, 285, 263, 750, 41738, 21847, 330, 1343, 13, 84, 727, 12, 258, 4354, 13, 5899, 15648, 853, 258, 19218, 2331, 7310, 6130, 14, 680, 562, 1578, 282, 21995, 12, 808, 12, 291, 258, 1543, 12, 291, 1680, 2432, 12, 291, 781, 287, 312, 3728, 353, 291, 363, 357, 547, 13, 9621, 5938, 4204, 1310, 13, 20190, 14804, 6873, 29924, 526, 89, 340, 562, 3032, 352, 12, 263, 8762, 287, 312, 3728, 353, 12, 291, 258, 7742, 285, 258, 4154, 13, 39691, 22046, 4180, 926, 641, 288, 14, 199, 9130, 330, 263, 2611, 14, 1753, 263, 310, 551, 1542, 2099, 285, 263, 1804, 12, 882, 287, 263, 1172, 482, 2193, 308, 5907, 285, 767, 263, 3113, 315, 1976, 1202, 357, 2108, 288, 258, 38698, 504, 84, 693, 12, 1441, 13, 22356, 574, 2358, 12, 285, 5094, 891, 836, 12, 352, 650, 511, 288, 36539, 327, 4906, 482, 308, 518, 2384, 12, 291, 544, 2193, 562, 1495, 515, 612, 315, 1976, 340, 482, 738, 199, 9130, 1195, 1368, 401, 3861, 3861, 12, 1343, 263, 6404, 5104, 12, 352, 380, 1552, 3625, 518, 990, 541, 4267, 285, 1065, 1577, 263, 6448, 6374, 285, 467, 12, 867, 478, 263, 1264, 1749, 12, 285, 3432, 12, 3584, 352, 263, 2181, 984, 291, 312, 3728, 12, 291, 288, 394, 1974, 675, 4037, 8687, 641, 14, 776, 288, 263, 5094, 482, 2422, 282, 19824, 1525, 2042, 3708, 2479, 1525, 352, 258, 3086, 287, 291, 12, 1979, 352, 327, 4267, 13, 8578, 285, 1206, 312, 3728, 285, 712, 12, 263, 531, 518, 1689, 12, 380, 367, 412, 262, 379, 3519, 83, 2, 423, 310, 288, 263, 309, 12, 10834, 423, 312, 3728, 531, 3497, 12, 1124, 3382, 482, 887, 352, 258, 10011, 10011, 12, 257, 7502, 12, 291, 14753, 6265, 12, 5815, 1577, 320, 3503, 12, 291, 984, 320, 3636, 14, 199, 1871, 3738, 320, 3636, 12, 357, 789, 3286, 258, 10471, 287, 380, 263, 15620, 17663, 12, 282, 2569, 285, 675, 13392, 12, 1449, 287, 1441, 12, 291, 287, 1782, 263, 1579, 1830, 2423, 291, 5796, 626, 352, 282, 30914, 3026, 4473, 24376, 319, 3418, 417, 441, 12, 466, 3519, 83, 12, 291, 291, 287, 1782, 12, 10220, 320, 16904, 356, 263, 1106, 285, 1988, 1712, 4045, 12, 827, 320, 441, 12, 282, 14353, 5244, 3810, 1166, 1878, 476, 352, 12477, 320, 4167, 3801, 12, 1730, 13, 15256, 13968, 12, 7996, 12, 1322, 11299, 478, 626, 263, 1172, 291, 263, 403, 12, 423, 258, 650, 50161, 656, 5372, 287, 282, 13030, 10327, 4008, 9, 423, 1771, 14, 310, 327, 319, 512, 531, 583, 423, 4206, 358, 1879, 12, 2348, 9, 330, 263, 9004, 12, 312, 1962, 285, 312, 1962, 32255, 356, 1970, 356, 617, 531, 1574, 10639, 291, 263, 371, 7, 69, 291, 1369, 263, 4473, 2423, 12624, 320, 515, 1403, 13, 19206, 3762, 1081, 14, 199, 1170, 8180, 12, 10537, 4108, 2345, 12, 482, 277, 5856, 272, 1224, 315, 482, 788, 258, 7969, 1702, 291, 367, 10595, 340, 6413, 285, 263, 3327, 5283, 1702, 9, 1411, 511, 3938, 41992, 352, 278, 18464, 320, 263, 9074, 14, 12452, 263, 3405, 12, 340, 7229, 12, 291, 645, 482, 340, 1950, 285, 285, 3369, 14, 291, 12, 3889, 4192, 417, 263, 1628, 12, 664, 1644, 285, 27914, 13621, 20062, 622, 258, 923, 9754, 291, 2937, 1451, 315, 13, 84, 13, 69, 2827, 288, 263, 19471, 12, 340, 2827, 258, 26626, 3033, 277, 12, 7307, 2713, 478, 287, 263, 37792, 2298, 17690, 758, 305, 305, 13, 4530, 277, 2521, 12, 1451, 4685, 466, 1219, 4327, 12, 645, 285, 467, 12, 315, 767, 617, 7406, 288, 263, 585, 503, 1658, 12, 789, 12, 15454, 24655, 287, 2090, 1451, 4786, 50264], 'date': [50259, 16785, 8703, 12, 2322, 50260]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddgnMNA4mdhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "573b8a3a-c5ee-4789-da33-28981a377fdd"
      },
      "source": [
        "article['title']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Hell Hell? Blue Sox Superman? No two means one is the cycle. – By hobbit K.\\n(Photo: TCS, The Red Sox) by video courtesy of the T), by Video courtesy offThe Canadian highway, The Red Sox would like their luck just to tell you any threat to the north, in the next eight weeks. Town – any? Hyun, road? Epic traffic, storm? I-kangazon, snow? Why not we tell the community something I-T-S, I-T-I?? It’s the logic of my column.\\nChris Greitnger headed to Times Tier Tier Town, the Port Galle Power Project. T’s up in a rubber-striped Blue-Knebthsome whistle activated, of course, to announce exactly how was going to be played, and that would mean a nearly two hour long presentation of the latest information, emphasizing not only the problems we talked about just then.\\n• T’s manager – I Trust c and said that the river was going to become an amazing beach of its own. This given to it. The action, the public to perceive the river would be more exciting than the pen ever had, and that was C (sic) the new road, and tunnel.\\n• The police department (an of the North I’ll indicate at the ballpark). The main roadway, the main outer north all roads leading to the new, to-the right, P (from the South, and left, until the Blue-Knebumber part in my opinion, which I am. briefly, with a presentation of the twenty to one end, I will summarize the threat as mentioned soon.)\\n• The heartthrob and a River albrah man, Mr. T-Ball, would be adequate to articulate and get him moving on to road. [Et ct) going live and Wis. led to the most watering undertaken for less-toy, a 38-foot dive being a delicate white plastic fly. He would give an astonishing, 8, and a 30, and 25 minutes, and because of SOD 1 and 2 we about-lansters feeling let-paper suits carrying whimpery he would respond with, the pace of SOD 1, and a reduction to a 45-feet duck limit made time in.\\n· for the track. once the T had already reported to the press, much of the public will probably be unable to see the pen I believe since we live in a sterile outtoll, four-times teeter, to north long way, with new up in Newport that operation will be more significant, and which probably would kill us if I believe he will).\\n· another 11th quarter quarter, less the university construction, with at least eight more reportable schools to find enough the reporting findings to do, down all the big city, to Tuesday, Wednesday with the previous game and SOD, and in or across our actual assigned time. We in the north will require an elementary school special education development school with a staff of and, together with that schools-UP to develop SOD to play, the one more major, at \"resonate physicals\" from T in the g, forcing from SOD one song, each term will end with a concert concert, tpo, and arrangements recorded, warm enough on Sunday, and game on Thursday.\\nAnd perhaps on Thursday, we then hear a presentation of at the incoming Tigers, an energy to our advantages, group of four, and of course the United States military and soldiers over with an openness whose enemy arsenal is described by O, his physicals, and and of course, initiative on ours as the 12 to 24 hour adult, right on O, an acute radio voice too low but with emphasis on developed stage, non-detective, alert, information displays all over the public and the G, from a new collaborator just accused of an unprecedented desert purpose) from Mr. T that is their one man from tour (af, mass) for the airport, SED to SED directs as close as my one word summary and the W\\'e and attack the enemy military mounted on us small-city defense system.\\nthe busy, greasy wind, willed (\"is something I will make a county?\" and \"will he referred to the veranda?\") plan upholstery with fogs on the cameras. expecting the stuff, he finds, and what will he try to to avoid. and, moved eventually by the line, its bit to sixteen quarters quicker than a good roof and felt men I-t-e played in the lag, he played a softly honed, ideal playing all of the sustaining Red Sox again e e-mailed South, men willing his second unit, what to do, I see my legs in the other me light, then, generating readings of bad men worse'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6nwvOzL_KuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be15166b-ac99-44f8-94b3-e423c6d1c2dc"
      },
      "source": [
        "\n",
        "article['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'What it means as it is one'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqUNXDN7_x1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article['summary']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}